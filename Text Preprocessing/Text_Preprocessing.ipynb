{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-K9q-s62gKU"
      },
      "source": [
        "# Przygotowanie danych tekstowych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDAKAYdi2gKY"
      },
      "source": [
        "W poniższym notatniku skupimy się głównie na przygotowaniu danych tekstowych - ich wektoryzacji, ekstrakcji cechy i czyszczeniu. Następnie przejdziemy do budowy modelu regresji logistycznej.\n",
        "\n",
        "Cały plik stanowi efekt postępów w projekcie pomiędzy 24.03.2022 a 21.04.2022. Najważniejsze punkty:\n",
        "1. Normalizacja tekstu\n",
        "2. Usunięcie z tekstu tzw. *stop words*\n",
        "3. Stemming tekstu za pomocą pakietu *SnowballStemmer*\n",
        "4. Lematyzacja słów komentarzy z użyciem biblioteki *spaCy*\n",
        "5. Wektoryzacja:\n",
        "    * aspekty teoretyczne (*Bag of words*, *N_gram*)\n",
        "    * wektoryzacja z użyciem pakietu TfidfVectorizer\n",
        "    * wektoryzacja alternatywna - stworzenie tensorów dwuwymiarowych w postaci zmiennych binarnych (eksperyment)\n",
        "6. Pierwsze próby budowania modelu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMwdORDN2gKY"
      },
      "outputs": [],
      "source": [
        "#wczytanie podstawowych pakietów\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9I4pNxZ2gKZ"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train.csv') #wczytanie zbioru danych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhW6aIBy2gKa"
      },
      "outputs": [],
      "source": [
        "train.drop(axis = 1, labels = \"id\", inplace = True) #id komentarzy nie będą nam potrzebne w dalszej klasyfikacji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ut3xvJ6T2gKa",
        "outputId": "27aa9b61-3308-4614-d4ab-ecc0ec0d3971"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text  toxic  severe_toxic  \\\n",
              "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
              "1  D'aww! He matches this background colour I'm s...      0             0   \n",
              "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
              "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
              "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
              "\n",
              "   obscene  threat  insult  identity_hate  \n",
              "0        0       0       0              0  \n",
              "1        0       0       0              0  \n",
              "2        0       0       0              0  \n",
              "3        0       0       0              0  \n",
              "4        0       0       0              0  "
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.head() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz6-8S7R2gKb"
      },
      "outputs": [],
      "source": [
        "#podział na zmienne objaśniające i objaśniane\n",
        "X_train = train.comment_text\n",
        "y_train = train.drop(axis = 1, labels = \"comment_text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9f0GXHe2gKc",
        "outputId": "a6571e78-87f9-4abb-e3eb-e7aaab8d0f6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         Explanation\\nWhy the edits made under my usern...\n",
              "1         D'aww! He matches this background colour I'm s...\n",
              "2         Hey man, I'm really not trying to edit war. It...\n",
              "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
              "4         You, sir, are my hero. Any chance you remember...\n",
              "                                ...                        \n",
              "159566    \":::::And for the second time of asking, when ...\n",
              "159567    You should be ashamed of yourself \\n\\nThat is ...\n",
              "159568    Spitzer \\n\\nUmm, theres no actual article for ...\n",
              "159569    And it looks like it was actually you who put ...\n",
              "159570    \"\\nAnd ... I really don't think you understand...\n",
              "Name: comment_text, Length: 159571, dtype: object"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G4TSt8_3c3Q",
        "outputId": "69da9d50-266a-4ffd-bdbf-aa354d07643c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The Mitsurugi point made no sense - why not argue to include Hindi on Ryo Sakazaki's page to include more information?\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "8rPsCOrB2gKc",
        "outputId": "e34f7fff-0067-40c3-80dc-058b83d53761"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159566</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159567</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159568</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159569</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159570</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159571 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
              "0           0             0        0       0       0              0\n",
              "1           0             0        0       0       0              0\n",
              "2           0             0        0       0       0              0\n",
              "3           0             0        0       0       0              0\n",
              "4           0             0        0       0       0              0\n",
              "...       ...           ...      ...     ...     ...            ...\n",
              "159566      0             0        0       0       0              0\n",
              "159567      0             0        0       0       0              0\n",
              "159568      0             0        0       0       0              0\n",
              "159569      0             0        0       0       0              0\n",
              "159570      0             0        0       0       0              0\n",
              "\n",
              "[159571 rows x 6 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8mMc1n92gKd"
      },
      "source": [
        "##  Stop words, normalizacja, stemming, lematyzacja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYeaQjAY2gKd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score , accuracy_score , confusion_matrix , f1_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91_mlkHI2gKe"
      },
      "source": [
        "## Normalizacja\n",
        "Normalizacja tekstu polega na takim jego przetworzeniu, aby miał spójną formę, która ułatwi dalszą interpretację tekstu (przykłady: zmiana liter na małe bądź wielkie, rozwinięcie skrótów, normalizacja skrótowców, konwersja wyrażeń numerycznych i wyrażeń słowno-numerycznych do postaci słownej, normalizacja znaków specjalnych – takich jak symbol akapitu czy znak zastrzeżenia prawa autorskiego, usunięcie lub zmiana znaków interpunkcyjnych itd.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIztQTF22gKe",
        "outputId": "fe2e8d1d-3984-4e01-f9fe-751f9f71f5bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I\r",
            "\r",
            "\r",
            "\r",
            "'am\n"
          ]
        }
      ],
      "source": [
        "#kod do normalizacji\n",
        "print(\"I\\r\\r\\r\\r'am\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtSMpbG32gKe"
      },
      "outputs": [],
      "source": [
        "def  clean_text(text):\n",
        "    text =  text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"\\r\", \"\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) \n",
        "    text = re.sub(\"(\\\\W)\",\" \",text) \n",
        "    text = re.sub('\\S*\\d\\S*\\s*','', text)\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha6pAz-m2gKf"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXnYMUdM2gKf",
        "outputId": "4ce43f68-d784-4dcb-9759-5ec6655f9d46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    explanation why the edits made under my userna...\n",
              "1    daww he matches this background colour i am se...\n",
              "2    hey man i am really not trying to edit war it ...\n",
              "3     more i cannot make any real suggestions on im...\n",
              "4    you sir are my hero any chance you remember wh...\n",
              "5      congratulations from me as well use the tool...\n",
              "6         cocksucker before you piss around on my work\n",
              "7    your vandalism to the matt shirvington article...\n",
              "8    sorry if the word nonsense was offensive to yo...\n",
              "9    alignment on this subject and which are contra...\n",
              "Name: comment_text, dtype: object"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzWj0lD12gKg"
      },
      "source": [
        "## Stop words\n",
        "Są to najczęściej występujące słowa języka, które na ogół nie niosą ze sobą żadnych istotnych treści. Są zatem zazwyczaj usuwane w celu optymalizacji modelu.\n",
        "\n",
        "W celu wykrycia i usunięcia wspomnianych słów skorzystamy z biblioteki *spaCy*, a konkretnie z anglojęzycznego pakietu *STOP_WORDS*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kza3PycB2gKg"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "nlp = English()\n",
        "# \"nlp\" Obiekt służy do tworzenia dokumentów z adnotacjami lingwistycznymi\n",
        "\n",
        "def stop_words_remove(text):\n",
        "    nlp_doc = nlp(text)\n",
        "\n",
        "    #tworzymi listę tokenów z przetwarzanego tekstu\n",
        "    tokens = []\n",
        "    for token in nlp_doc:\n",
        "        tokens.append(token.text)\n",
        "\n",
        "    # usuwamy 'stop words' i tworzymy nową listę tokenów\n",
        "    filtered_tokens =[] \n",
        "\n",
        "    for word in tokens:\n",
        "        word_id = nlp.vocab[word]\n",
        "        if word_id.is_stop == False:\n",
        "            filtered_tokens.append(word)\n",
        "    \n",
        "    filtered_comment = ' '.join(filtered_tokens) \n",
        "    \n",
        "    return filtered_comment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0c1ZqY_3c3S"
      },
      "source": [
        "**Przykład działania**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A07U1YPv3c3S",
        "outputId": "56f05def-5a79-42fe-dd80-11dcd13b33a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sorry if the word nonsense was offensive to you anyway i am not intending to write anything in the articlewow they would jump on me for vandalism i am merely requesting that it be more encyclopedic so one can use it for school as a reference i have been to the selective breeding page but it is almost a stub it points to animal breeding which is a short messy article that gives you no info there must be someone around with expertise in eugenics '"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "LilmUzt43c3T",
        "outputId": "71333af1-8a98-450f-e9d2-b7dad827e4a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sorry word nonsense offensive intending write articlewow jump vandalism merely requesting encyclopedic use school reference selective breeding page stub points animal breeding short messy article gives info expertise eugenics'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words_remove(X_train[8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76oIl1jH3c3T"
      },
      "source": [
        "**Usunięcie '*stop words*' ze zbioru treningowego**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbetRoun3c3T",
        "outputId": "41688a4f-d444-4233-9297-1ec92ffabb60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         explanation edits username hardcore metallica ...\n",
              "1         daww matches background colour seemingly stuck...\n",
              "2         hey man trying edit war guy constantly removin...\n",
              "3           real suggestions improvement   wondered sect...\n",
              "4                             sir hero chance remember page\n",
              "                                ...                        \n",
              "159566    second time asking view completely contradicts...\n",
              "159567                ashamed    horrible thing talk page  \n",
              "159568    spitzer    umm s actual article prostitution r...\n",
              "159569      looks like actually speedy version deleted look\n",
              "159570        think understand   came idea bad right awa...\n",
              "Name: comment_text, Length: 159571, dtype: object"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_stop = X_train.apply(stop_words_remove)\n",
        "X_train_stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8lCxwK12gKg"
      },
      "source": [
        "## Stemming \n",
        "Stemming jest procesem usunięcia końcówki fleksyjnej ze słowa, w  czego efekcie pozostaje tylko temat wyrazu. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhsBSEDY2gKg"
      },
      "outputs": [],
      "source": [
        "#nltk.download('stopwords')\n",
        "sn = SnowballStemmer(language='english')\n",
        "\n",
        "\n",
        "def stemmer(text):\n",
        "    words =  text.split()\n",
        "    train = [sn.stem(word) for word in words if not word in set(stopwords.words('english'))]\n",
        "    return ' '.join(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAHbV17r2gKh"
      },
      "outputs": [],
      "source": [
        "X_train_stem = X_train_stop.apply(stemmer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWy3NxGY2gKh"
      },
      "outputs": [],
      "source": [
        "# żeby uniknąć każdorazowego uruchamiania stemmingu zapisujemy wynikową tabelę do pliku .csv\n",
        "X_train_stem = pd.DataFrame(X_train_stem)\n",
        "X_train_stem.to_csv('X_train_stem.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2RGJhRi3c3U",
        "outputId": "76be7796-5283-4521-f307-33a664135157"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         explan edit made usernam hardcor metallica fan...\n",
              "1         daww match background colour seem stuck thank ...\n",
              "2         hey man realli tri edit war guy constant remov...\n",
              "3         cannot make real suggest improv wonder section...\n",
              "4                                sir hero chanc rememb page\n",
              "                                ...                        \n",
              "159566    second time ask view complet contradict covera...\n",
              "159567                    asham horribl thing put talk page\n",
              "159568    spitzer umm there actual articl prostitut ring...\n",
              "159569    look like actual put speedi first version dele...\n",
              "159570    realli think understand came idea bad right aw...\n",
              "Name: comment_text, Length: 159571, dtype: object"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stem_csv = pd.read_csv('X_train_stem.csv')['comment_text']\n",
        "stem_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pgkRHoa2gKh"
      },
      "source": [
        "## Lematyzacja\n",
        "\n",
        "Lematyzacja to sprowadzenie słowa do jego podstawowej postaci. Na przykład w przypadku czasownika to najczęściej będzie bezokolicznik, w przypadku rzeczownika sprowadzamy do mianownika liczby pojedynczej."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKGoUpfW2gKh"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "#budujemy model\n",
        "#NER - Named Entity Recognition - wyłączamy\n",
        "#'parser' daje informacje składniowe - póki co ich nie potrzebujemy. \n",
        "# https://spacy.io/usage/linguistic-features#disabling\n",
        "\n",
        "# Wyłączenie parsera sprawi, że SpaCy będzie ładował się i działał znacznie szybciej\n",
        "# https://spacy.io/usage/linguistic-features#named-entities\n",
        "load_model = spacy.load('en_core_web_sm', disable = ['parser','ner'])\n",
        "\n",
        "def lemmatization(text):\n",
        "    text_model = load_model(text)\n",
        "    result = \" \".join([token.lemma_ for token in text_model])\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H7216WG3c3U"
      },
      "outputs": [],
      "source": [
        "X_train_lem = X_train_stop.apply(lemmatization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DnMgouZ3c3V"
      },
      "source": [
        "**Przykład działania**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kw5pO7P3c3V",
        "outputId": "1a50a4d7-7780-41b9-cbef-7ce501f21600"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'explanation edits username hardcore metallica fan reverted vandalisms closure gas voted new york dolls fac remove template talk page retired'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_stop[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx-scKEr3c3V",
        "outputId": "abec2094-4763-44b4-f746-9a92457b9b40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'explanation edit username hardcore metallica fan revert vandalism closure gas vote new york doll fac remove template talk page retire'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_lem[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaQEkSFx3c3V"
      },
      "outputs": [],
      "source": [
        "# żeby uniknąć każdorazowego uruchamiania funkcji z lematyzacją zapisujemy wynikową tabelę do pliku .csv\n",
        "X_train_lemm = pd.DataFrame(X_train_lem)\n",
        "X_train_lemm.to_csv('X_train_lem.csv') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kph-h8J62gKi"
      },
      "source": [
        "## Wektoryzacja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faGartb62gKi"
      },
      "source": [
        "Na początku przedstawimy kilka podstawowych pojęć związanych z przetwarzaniem języka naturalnego.\n",
        "### „Bag of words” \n",
        "Aby algorytm mógł sobie poradzić z tekstem, musimy najpierw podzielić ten tekst na mniejsze fragmenty. Stworzenie tzw. \"worka słów\" jest jednym ze sposób na uzyskanie takiego podziału. Każde słowo użyte w tekście zostaje wyodrębnione i wrzucone do multizbioru. Dla przykładu, jeśli mamy dwa zdania: „Marcin ma kota.” oraz „Patryk ma psa.”, to w worku słów znajdzie się pięć słów: Marcin, ma, kota, Patryk, psa. Ich kolejność nie będzie odgrywała roli.\n",
        "\n",
        "\n",
        "### N_gram\n",
        "W naszym worku mogą się znaleźć nietylko pojedyńcze słowa, ale pewne sekwencje słów. N-gram jest ciągiem elementów z danej próbki tekstu bądź mowy. Zazwyczaj jednym elementem jest pojedyncze słowo (ale w określonych przypadkach mogą też być to fonemy, litery lub sylaby).\n",
        "\n",
        "### Wektoryzacja\n",
        "\n",
        "Z \"bag of words\" blisko związanym terminem jest wektoryzacjia. Najprostszym wektoryzatorem jest CountVectorizer. Zlicza on liczbę wystąpień każdego wyrazu (lub n_gramu) w tekście i przedstawia za pomocą wektora składającego się z liczb naturalnych. Każda liczba informuje, ile razy dany element wystąpił w analizowanym tekście.\n",
        "\n",
        "Przykładem innego wektoryzatora, który rozpatrywaliśmy w naszym projekcie, jest wektoryzator TF-IDF. Opiera się on na metodzie obliczania wagi słów w oparciu o liczbę ich wystąpień w całym zbiorze jak i w pojedyńczych dokumentach.\n",
        "\n",
        "$$\\text{Dla termu }t_i\\text{ w dokumencie }d_j\\text{ mamy:}$$\n",
        "\n",
        "$$(tf-idf)_{i,j} = (tf)_{i,j} \\times (idf)_{i}$$\n",
        "gdzie:\n",
        "\n",
        "$(tf)_{i,j}-$ term frequency, liczba wystąpień termu $t_i$ w dokumencie $d_j$ podzielona przez liczbę wszystkich termów w $d_j.$\n",
        "\n",
        "$(idf)_i-$ inverse document frequency, $(idf)_i = ln\\left( \\dfrac{|D|}{|\\{j\\ : f_i \\in d_j\\}|} \\right)$ - gdzie $D$ to zbiór wszystkich dokumentów.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls2NYX5x2gKi"
      },
      "outputs": [],
      "source": [
        "word_vectorizer = TfidfVectorizer(\n",
        "    strip_accents='unicode', #normalizacja tekstu, usuwanie akcentów itp. unicode jest wolniejsza, ale radzi sobie z dowolnymi znakami   \n",
        "    token_pattern=r'\\w{1,}',  #co zaliczamy jako token - tutaj są to obiekty typu r'\\w' czyli o kategorii alfabetonumerycznej, o długości 1 lub większej   \n",
        "    ngram_range=(1, 3),      #liczba możliwych n-gramów - tutaj dopuszczamy mono-, bi-, i tri-gramy  \n",
        "    stop_words='english', #jaka kategoria dla stopwords, domyślnie jest None, dostępna jest opcja 'english' lub inna własna lista\n",
        "    sublinear_tf=True) #zamiast term frequency (tf) oddaje 1+ln(tf)\n",
        "\n",
        "word_vectorizer.fit(X_train)    \n",
        "train_word_features = word_vectorizer.transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKJFubrH2gKi"
      },
      "outputs": [],
      "source": [
        "X_train_transformed = word_vectorizer.transform(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LBwZzuY2gKj"
      },
      "outputs": [],
      "source": [
        "print(X_train_transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkEDjeI62gKj"
      },
      "source": [
        "źródła: https://www.statystyczny.pl/klasyfikacja-tekstu-text-classification/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D49HSXSy3c3W"
      },
      "source": [
        "### Wektoryzacja - tensor ze zmiennymi binarnymi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yUJNe_j3c3W"
      },
      "source": [
        "Alternatywnym sposobem wektoryzacji będzie stworzenie tensora ze zmiennymi zero-jedynkowymi. Każda z nich odpowiada za wystąpienie bądź niewystąpienie danego słowa w danym komentarzu. Postać binarna macierzy powinna zapobiec problemowi zbyt dużej złożoności obliczeniowej.\n",
        "\n",
        "Jednak żeby uniknąć macierzy o zbyt dużych wymiarach wybierzemy jedynie 1000 najczęściej występujących słów w zbiorze. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MneBuLfJ3c3W"
      },
      "outputs": [],
      "source": [
        "full_text = ' '.join(list(X_train_lem))\n",
        "full_text = full_text.split()\n",
        "words = pd.DataFrame(full_text).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "uOZXWfVh3c3W",
        "outputId": "516f05ff-2f80-43c1-c981-b8d6c1d3483b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        word  count\n",
            "0    article  72932\n",
            "1       page  56447\n",
            "2  wikipedia  37072\n",
            "3       edit  36560\n",
            "4       talk  33962\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array(['article', 'page', 'wikipedia', ..., 'resonance', 'intact',\n",
              "       'physician'], dtype=object)"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words_5000 = words.rename_axis(['word'])\\\n",
        "            .reset_index()\\\n",
        "            .rename(columns={0:'count'})\\\n",
        "            .sort_values(by=['count'], ascending=False)\n",
        "\n",
        "words_sequence = words_5000.loc[0:5000,'word'].values\n",
        "words_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uIbzHV23c3W"
      },
      "outputs": [],
      "source": [
        "def vectorize_sequences(dataset, sequences, dimension=1000):\n",
        "    results = np.zeros(shape = (dataset.shape[0], dimension), dtype=int)\n",
        "    for j in range(dataset.shape[0]):\n",
        "        for i,sequence in enumerate(sequences[0:1000]):\n",
        "            if sequence in dataset[j]:\n",
        "                results[j,i] = 1\n",
        "                \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O745hFWa3c3X"
      },
      "outputs": [],
      "source": [
        "#X_train_vect_bin = vectorize_sequences(X_train_bin_vect, list(words_sequence))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "text_preprocessing_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}