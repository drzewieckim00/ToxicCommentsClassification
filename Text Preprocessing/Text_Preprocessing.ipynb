{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-K9q-s62gKU"
   },
   "source": [
    "# Przygotowanie danych tekstowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDAKAYdi2gKY"
   },
   "source": [
    "W poniższym notatniku skupimy się głównie na przygotowaniu danych tekstowych - ich wektoryzacji, ekstrakcji cechy i czyszczeniu. Następnie przejdziemy do budowy modelu regresji logistycznej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AMwdORDN2gKY"
   },
   "outputs": [],
   "source": [
    "#wczytanie podstawowych pakietów\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z9I4pNxZ2gKZ"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv') #wczytanie zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yhW6aIBy2gKa"
   },
   "outputs": [],
   "source": [
    "train.drop(axis = 1, labels = \"id\", inplace = True) #id komentarzy nie będą nam potrzebne w dalszej klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Ut3xvJ6T2gKa",
    "outputId": "27aa9b61-3308-4614-d4ab-ecc0ec0d3971"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \n",
       "0        0       0       0              0  \n",
       "1        0       0       0              0  \n",
       "2        0       0       0              0  \n",
       "3        0       0       0              0  \n",
       "4        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uz6-8S7R2gKb"
   },
   "outputs": [],
   "source": [
    "#podział na zmienne objaśniające i objaśniane\n",
    "X_train = train.comment_text\n",
    "y_train = train.drop(axis = 1, labels = \"comment_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9f0GXHe2gKc",
    "outputId": "a6571e78-87f9-4abb-e3eb-e7aaab8d0f6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Explanation\\nWhy the edits made under my usern...\n",
       "1         D'aww! He matches this background colour I'm s...\n",
       "2         Hey man, I'm really not trying to edit war. It...\n",
       "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4         You, sir, are my hero. Any chance you remember...\n",
       "                                ...                        \n",
       "159566    \":::::And for the second time of asking, when ...\n",
       "159567    You should be ashamed of yourself \\n\\nThat is ...\n",
       "159568    Spitzer \\n\\nUmm, theres no actual article for ...\n",
       "159569    And it looks like it was actually you who put ...\n",
       "159570    \"\\nAnd ... I really don't think you understand...\n",
       "Name: comment_text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "8rPsCOrB2gKc",
    "outputId": "e34f7fff-0067-40c3-80dc-058b83d53761"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0           0             0        0       0       0              0\n",
       "1           0             0        0       0       0              0\n",
       "2           0             0        0       0       0              0\n",
       "3           0             0        0       0       0              0\n",
       "4           0             0        0       0       0              0\n",
       "...       ...           ...      ...     ...     ...            ...\n",
       "159566      0             0        0       0       0              0\n",
       "159567      0             0        0       0       0              0\n",
       "159568      0             0        0       0       0              0\n",
       "159569      0             0        0       0       0              0\n",
       "159570      0             0        0       0       0              0\n",
       "\n",
       "[159571 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8mMc1n92gKd"
   },
   "source": [
    "##  Stop words, normalizacja, stemming, lematyzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gYeaQjAY2gKd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score , accuracy_score , confusion_matrix , f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91_mlkHI2gKe"
   },
   "source": [
    "### Normalizacja\n",
    "Normalizacja tekstu polega na takim jego przetworzeniu, aby miał spójną formę, która ułatwi dalszą interpretację tekstu (przykłady: zmiana liter na małe bądź wielkie, rozwinięcie skrótów, normalizacja skrótowców, konwersja wyrażeń numerycznych i wyrażeń słowno-numerycznych do postaci słownej, normalizacja znaków specjalnych – takich jak symbol akapitu czy znak zastrzeżenia prawa autorskiego, usunięcie lub zmiana znaków interpunkcyjnych itd.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mIztQTF22gKe"
   },
   "outputs": [],
   "source": [
    "#kod do normalizacji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BtSMpbG32gKe"
   },
   "outputs": [],
   "source": [
    "def  clean_text(text):\n",
    "    text =  text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"\\r\", \"\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "    text = re.sub(\"(\\\\W)\",\" \",text) \n",
    "    text = re.sub('\\S*\\d\\S*\\s*','', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ha6pAz-m2gKf"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXnYMUdM2gKf",
    "outputId": "4ce43f68-d784-4dcb-9759-5ec6655f9d46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    explanation why the edits made under my userna...\n",
       "1    daww he matches this background colour i am se...\n",
       "2    hey man i am really not trying to edit war it ...\n",
       "3     more i cannot make any real suggestions on im...\n",
       "4    you sir are my hero any chance you remember wh...\n",
       "5      congratulations from me as well use the tool...\n",
       "6         cocksucker before you piss around on my work\n",
       "7    your vandalism to the matt shirvington article...\n",
       "8    sorry if the word nonsense was offensive to yo...\n",
       "9    alignment on this subject and which are contra...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzWj0lD12gKg"
   },
   "source": [
    "### Stop words\n",
    "Są to najczęściej występujące słowa języka, które na ogół nie niosą ze sobą żadnych istotnych treści. Są zatem zazwyczaj usuwane w celu optymalizacji modelu.\n",
    "\n",
    "W celu wykrycia i usunięcia wspomnianych słów skorzystamy z biblioteki *spaCy*, a konkretnie z anglojęzycznego pakietu *STOP_WORDS*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "kza3PycB2gKg"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "def stop_words_remove(text):\n",
    "    nlp_doc = nlp(text)\n",
    "\n",
    "    #tworzymi listę tokenów z przetwarzanego tekstu\n",
    "    tokens = []\n",
    "    for token in nlp_doc:\n",
    "        tokens.append(token.text)\n",
    "\n",
    "    # usuwamy 'stop words' i tworzymy nową listę tokenów\n",
    "    filtered_tokens =[] \n",
    "\n",
    "    for word in tokens:\n",
    "        word_id = nlp.vocab[word]\n",
    "        if word_id.is_stop == False:\n",
    "            filtered_tokens.append(word)\n",
    "    \n",
    "    filtered_comment = ' '.join(filtered_tokens) \n",
    "    \n",
    "    return filtered_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Przykład działania**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sorry if the word nonsense was offensive to you anyway i am not intending to write anything in the articlewow they would jump on me for vandalism i am merely requesting that it be more encyclopedic so one can use it for school as a reference i have been to the selective breeding page but it is almost a stub it points to animal breeding which is a short messy article that gives you no info there must be someone around with expertise in eugenics '"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sorry word nonsense offensive intending write articlewow jump vandalism merely requesting encyclopedic use school reference selective breeding page stub points animal breeding short messy article gives info expertise eugenics'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_remove(X_train[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usunięcie '*stop words*' ze zbioru treningowego**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         explanation edits username hardcore metallica ...\n",
       "1         daww matches background colour seemingly stuck...\n",
       "2         hey man trying edit war guy constantly removin...\n",
       "3           real suggestions improvement   wondered sect...\n",
       "4                             sir hero chance remember page\n",
       "                                ...                        \n",
       "159566    second time asking view completely contradicts...\n",
       "159567                ashamed    horrible thing talk page  \n",
       "159568    spitzer    umm s actual article prostitution r...\n",
       "159569      looks like actually speedy version deleted look\n",
       "159570        think understand   came idea bad right awa...\n",
       "Name: comment_text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stop = X_train.apply(stop_words_remove)\n",
    "X_train_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8lCxwK12gKg"
   },
   "source": [
    "### Stemming \n",
    "Stemming jest procesem usunięcia końcówki fleksyjnej ze słowa, w  czego efekcie pozostaje tylko temat wyrazu. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhsBSEDY2gKg",
    "outputId": "1756a234-dab8-48ee-f370-d105564894d1"
   },
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "sn = SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "def stemmer(text):\n",
    "    words =  text.split()\n",
    "    train = [sn.stem(word) for word in words if not word in set(stopwords.words('english'))]\n",
    "    return ' '.join(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "SAHbV17r2gKh",
    "outputId": "4bd08a93-55dd-4127-9a43-09c8ad1a7499"
   },
   "outputs": [],
   "source": [
    "X_train_lem = X_train.apply(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWy3NxGY2gKh"
   },
   "outputs": [],
   "source": [
    "X_train_stem = pd.DataFrame(X_train2)\n",
    "X_train_stem.to_csv('X_train_stem.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pgkRHoa2gKh"
   },
   "source": [
    "### Lematyzacja\n",
    "\n",
    "Lematyzacja to sprowadzenie słowa do jego podstawowej postaci. Na przykład w przypadku czasownika to najczęściej będzie bezokolicznik, w przypadku rzeczownika sprowadzamy do mianownika liczby pojedynczej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "zKGoUpfW2gKh"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "#budujemy model\n",
    "#NER - Named Entity Recognition - wyłączamy\n",
    "#'parser' daje informacje składniowe - póki co ich nie potrzebujemy. \n",
    "# https://spacy.io/usage/linguistic-features#disabling\n",
    "\n",
    "# Wyłączenie parsera sprawi, że SpaCy będzie ładował się i działał znacznie szybciej\n",
    "# https://spacy.io/usage/linguistic-features#named-entities\n",
    "load_model = spacy.load('en_core_web_sm', disable = ['parser','ner'])\n",
    "\n",
    "def lemmatization(text):\n",
    "    text_model = load_model(text)\n",
    "    result = \" \".join([token.lemma_ for token in text_model])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lem = X_train.apply(lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Przykład działania**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation why the edit make under my username hardcore metallica fan be revert they be not vandalism just closure on some gas after I vote at new york doll fac and please do not remove the template from the talk page since I be retire'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemm = pd.DataFrame(X_train_lem)\n",
    "X_train_lemm.to_csv('X_train_lem.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kph-h8J62gKi"
   },
   "source": [
    "## Wektoryzacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faGartb62gKi"
   },
   "source": [
    "Na początku przedstawimy kilka podstawowych pojęć związanych z przetwarzaniem języka naturalnego.\n",
    "### „Bag of words” \n",
    "Aby algorytm mógł sobie poradzić z tekstem, musimy najpierw podzielić ten tekst na mniejsze fragmenty. Stworzenie tzw. \"worka słów\" jest jednym ze sposób na uzyskanie takiego podziału. Każde słowo użyte w tekście zostaje wyodrębnione i wrzucone do multizbioru. Dla przykładu, jeśli mamy dwa zdania: „Marcin ma kota.” oraz „Patryk ma psa.”, to w worku słów znajdzie się pięć słów: Marcin, ma, kota, Patryk, psa. Ich kolejność nie będzie odgrywała roli.\n",
    "\n",
    "\n",
    "### N_gram\n",
    "W naszym worku mogą się znaleźć nietylko pojedyńcze słowa, ale pewne sekwencje słów. N-gram jest ciągiem elementów z danej próbki tekstu bądź mowy. Zazwyczaj jednym elementem jest pojedyncze słowo (ale w określonych przypadkach mogą też być to fonemy, litery lub sylaby).\n",
    "\n",
    "### Wektoryzacja\n",
    "\n",
    "Z \"bag of words\" blisko związanym terminem jest wektoryzacjia. Najprostszym wektoryzatorem jest CountVectorizer. Zlicza on liczbę wystąpień każdego wyrazu (lub n_gramu) w tekście i przedstawia za pomocą wektora składającego się z liczb naturalnych. Każda liczba informuje, ile razy dany element wystąpił w analizowanym tekście.\n",
    "\n",
    "!!! do zrobienia notatka czym się różni count od tf-idf, dlaczego będziemy go używać, możemy też zrobić testy jak sobie poradził count, ale to chyba przesada by była xd\n",
    "\n",
    "https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ls2NYX5x2gKi"
   },
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    strip_accents='unicode', #normalizacja tekstu, usuwanie akcentów itp. unicode jest wolniejsza, ale radzi sobie z dowolnymi znakami   \n",
    "    token_pattern=r'\\w{1,}',  #co zaliczamy jako token - tutaj są to obiekty typu r'\\w' czyli o kategorii alfabetonumerycznej, o długości 1 lub większej   \n",
    "    ngram_range=(1, 3),      #liczba możliwych n-gramów - tutaj dopuszczamy mono-, bi-, i tri-gramy  \n",
    "    stop_words='english', #jaka kategoria dla stopwords, domyślnie jest None, dostępna jest opcja 'english' lub inna własna lista\n",
    "    sublinear_tf=True) #zamiast term frequency (tf) oddaje 1+ln(tf)\n",
    "\n",
    "word_vectorizer.fit(X_train)    \n",
    "train_word_features = word_vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKJFubrH2gKi"
   },
   "outputs": [],
   "source": [
    "X_train_transformed = word_vectorizer.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LBwZzuY2gKj"
   },
   "outputs": [],
   "source": [
    "print(X_train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkEDjeI62gKj"
   },
   "source": [
    "źródła: https://www.statystyczny.pl/klasyfikacja-tekstu-text-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wektoryzacja - tensor ze zmiennymi binarnymi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatywnym sposobem wektoryzacji będzie stworzenie tensora ze zmiennymi zero-jedynkowymi. Każda z nich odpowiada za wystąpienie bądź niewystąpienie danego słowa w danym komentarzu. Postać binarna macierzy powinna zapobiec problemowi zbyt dużej złożoności obliczeniowej.\n",
    "\n",
    "Jednak żeby uniknąć macierzy o zbyt dużych wymiarach wybierzemy jedynie 1000 najczęściej występujących słów w zbiorze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         explanation edit username hardcore metallica f...\n",
       "1         daww match background colour seemingly stuck t...\n",
       "2         hey man try edit war guy constantly remove rel...\n",
       "3            real suggestion improvement    wonder secti...\n",
       "4                             sir hero chance remember page\n",
       "                                ...                        \n",
       "159566    second time ask view completely contradict cov...\n",
       "159567               ashamed     horrible thing talk page  \n",
       "159568    spitzer     umm s actual article prostitution ...\n",
       "159569        look like actually speedy version delete look\n",
       "159570          think understand    come idea bad right ...\n",
       "Name: comment_text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bin_vect = X_train_lem.apply(stop_words_remove)\n",
    "X_train_bin_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = ' '.join(list(X_train_bin_vect))\n",
    "full_text = full_text.split()\n",
    "words = pd.DataFrame(full_text).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  count\n",
      "0    article  72932\n",
      "1       page  56447\n",
      "2  wikipedia  37072\n",
      "3       edit  36560\n",
      "4       talk  33962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['article', 'page', 'wikipedia', ..., 'resonance', 'intact',\n",
       "       'physician'], dtype=object)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_5000 = words.rename_axis(['word'])\\\n",
    "            .reset_index()\\\n",
    "            .rename(columns={0:'count'})\\\n",
    "            .sort_values(by=['count'], ascending=False)\n",
    "\n",
    "print(words_5000.head())\n",
    "words_sequence = words_5000.loc[0:5000,'word'].values\n",
    "words_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(dataset, sequences, dimension=1000):\n",
    "    results = np.zeros(shape = (dataset.shape[0], dimension), dtype=int)\n",
    "    for j in range(dataset.shape[0]):\n",
    "        for i,sequence in enumerate(sequences[0:1000]):\n",
    "            if sequence in dataset[j]:\n",
    "                results[j,i] = 1\n",
    "                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chyba jednak zbyt duza ta tablica\n",
    "#X_train_vect_bin = vectorize_sequences(X_train_bin_vect, list(words_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM4AbAkh2gKj"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9e3xZbUF2gKj"
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C = 10, penalty='l2', solver = 'liblinear', random_state=45)\n",
    "\n",
    "classifier = OneVsRestClassifier(log_reg)\n",
    "classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "y_train_pred_proba = classifier.predict_proba(X_train_transformed)\n",
    "y_test_pred_proba = classifier.predict_proba(X_test_transformed)\n",
    "\n",
    "\n",
    "roc_auc_score_train = roc_auc_score(y_train, y_train_pred_proba,average='weighted')\n",
    "roc_auc_score_test = roc_auc_score(y_test, y_test_pred_proba,average='weighted')\n",
    "\n",
    "print(\"ROC AUC Score Train:\", roc_auc_score_train)\n",
    "print(\"ROC AUC Score Test:\", roc_auc_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eq6QGD282gKj"
   },
   "outputs": [],
   "source": [
    "def make_test_predictions(df,classifier):\n",
    "    df.comment_text = df.comment_text.apply(clean_text)\n",
    "    df.comment_text = df.comment_text.apply(stemmer)\n",
    "    X_test = df.comment_text\n",
    "    X_test_transformed = vectorize.transform(X_test)\n",
    "    y_test_pred = classifier.predict_proba(X_test_transformed)\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZazxnYDyumF"
   },
   "outputs": [],
   "source": [
    "xx ={'id':[565],'comment_text':['Steve is Steve is Steve']}\n",
    "xx = pd.DataFrame(xx)\n",
    "make_test_predictions(xx,classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_preprocessing (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
